{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B\n",
    "\n",
    "Instructions are given in <span style=\"color:blue\">blue</span> color.\n",
    "(Unfortunately, Google Colab won't display the blue color.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This task is a continuation of task A. Please make sure that you have completed it in advance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Limericks\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "# force gpu computing, when gpu library is available\n",
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_AVAILABLE = True\n",
      "Doing inference with the trained networks using cuda\n"
     ]
    }
   ],
   "source": [
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "print(f'CUDA_AVAILABLE = {CUDA_AVAILABLE}')\n",
    "\n",
    "# Set the device for inference\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and USE_GPU else \"cpu\")\n",
    "print(f'Doing inference with the trained networks using {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "limericks_file = \"Data/limericks.txt\"\n",
    "limericks = open(limericks_file, 'r').read()\n",
    "vocab = sorted(set(limericks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating unique index for each character in the text\n",
    "char_to_index = {char:index for index, char in enumerate(vocab)}\n",
    "# Mapping the index back to the characters\n",
    "index_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary length\n",
    "vocab_size = len(vocab)\n",
    "# Embedding dimensions\n",
    "embed_dim = 64\n",
    "# Number of RNN units (neurons)\n",
    "rnn_units = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <div style=\"color:blue\">Define the same Network Class as in Task A.</div>\n",
    "* <div style=\"color:blue\">Load both your trained models into separate model objects and move them to the desired device.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, recurrent_type='LSTM'):\n",
    "        super(Network, self).__init__()\n",
    "        self.recurrent_type = recurrent_type\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if recurrent_type == 'RNN':\n",
    "            self.rnn = nn.RNN(embedding_dim, rnn_units, batch_first=True)\n",
    "        elif recurrent_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, rnn_units, batch_first=True)\n",
    "        elif recurrent_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, rnn_units, batch_first=True)\n",
    "        self.linear = nn.Linear(rnn_units, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        #lstm also returns the cell hidden state\n",
    "        if self.recurrent_type == \"LSTM\":\n",
    "            output, (hidden, cell_hidden) = self.rnn(embedded)\n",
    "        else:\n",
    "            output, hidden = self.rnn(embedded)\n",
    "        output = self.linear(hidden[-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (embedding): Embedding(70, 64)\n",
       "  (rnn): LSTM(64, 512, batch_first=True)\n",
       "  (linear): Linear(in_features=512, out_features=70, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_v1 = Network(vocab_size, embed_dim, rnn_units, recurrent_type='LSTM')\n",
    "model_v1.load_state_dict(torch.load(\"Models/network.pt\"))\n",
    "model_v1.to(device)\n",
    "model_v2 = Network(vocab_size, embed_dim, rnn_units, recurrent_type='LSTM')\n",
    "model_v2.load_state_dict(torch.load(\"Models/network_v2.pt\"))\n",
    "model_v2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate text based on a given start sequence\n",
    "def generate_limericks(model, start_sequence, sequence_length, length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    \n",
    "    encoded_input = [char_to_index[s] for s in start_sequence]\n",
    "    encoded_input = torch.tensor(encoded_input).view(1, -1).to(device)\n",
    "\n",
    "    generated_str = start_sequence\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(length):\n",
    "            logits = model(encoded_input)\n",
    "            logits = logits.squeeze(0)\n",
    "\n",
    "            scaled_logits = logits * temperature\n",
    "            new_char_indx = torch.multinomial(torch.exp(scaled_logits), num_samples=1)\n",
    "\n",
    "            new_char_indx = new_char_indx[-1].item()\n",
    "\n",
    "            generated_str += str(index_to_char[new_char_indx])\n",
    "\n",
    "            new_char_indx = torch.tensor([new_char_indx]).view(1, 1).to(device)\n",
    "            encoded_input = torch.cat(\n",
    "                [encoded_input, new_char_indx],\n",
    "                dim=1)\n",
    "            encoded_input = encoded_input[:, -sequence_length:]\n",
    "\n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <div style=\"color:blue\">Use your models and the provided helper function <code>generate_limericks</code> to create some text. Evaluate the quality of both models. Were they able to replicate the structure of a limerick? What about rhyme schemes, grammar, and content?</br>\n",
    "<b>Hint:</b> You might want to play around with the <code>length</code> parameter when generating the limericks</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from the mornings:\n",
      "he's a prade of the wart!\n",
      "i'm a braggle's fhawly, but to rail.\n",
      "it here's takes bo jonet,\n",
      "sof a mans he have comprengations,\n",
      "sumery is the persoze in underd.\n",
      "\n",
      "uftern the aisus only, crocks as his dui\n"
     ]
    }
   ],
   "source": [
    "print(generate_limericks(model_v1, \"from the mornings\", 60, length=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from the mornings,\n",
      "was ans clearing a starting the back.\n",
      "but the courtrips should never set.\n",
      "i am troubles flowers, to call \"cow,\n",
      "at you're taking off and a wide;\n",
      "on the critimes, make you're cace them call,\" says he strong.\n",
      "\n",
      "there once was a figure mom's severe,\n",
      "they clue down the opening break\n",
      "than a colon atome word!\n",
      "it is sure things he'd been the bones.\n",
      "\n",
      "an amara croop, self-won't corned\n",
      "to a strught up, i'm earling thicks.\n",
      "in no mates on emotion\n",
      "and fresh them out; my divine,\n",
      "while the audus still ways:\n",
      "i've been found the divine,\n",
      "not a buildifi may\n",
      "excell and starts them all chicken money-\n",
      "he'd be noans on embroar and spine.\n",
      "\n",
      "to citious holding about,\n",
      "it's a profusting around (it ?\n",
      "my moon, the ole-type god's show!\n",
      "in the soul was provide to a crime.\n",
      "give the store of a practive didn't bell!\"\n",
      "\"you can see a searlict matter.\n",
      "though the season with stops,\n",
      "may be caused his great piece of preciss.\n",
      "\n",
      "\"let no limerick (i'd pressed \"but your crimbed by the small.\n",
      "\n",
      "have a minuter-but known as a passion \n"
     ]
    }
   ],
   "source": [
    "print(generate_limericks(model_v2, \"from the mornings\", 60, length=1000, temperature=1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has obviously been able to learn some of the structure of a limerick, although it has not yet gotten that there are always five lines. The TLM cannot produce sentences which make sense, and it has not picked up on rhyming either (might be something for LLMs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
